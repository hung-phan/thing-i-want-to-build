{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_encode(text, vocab):\n",
    "    return [vocab.index(x) + 1 for x in text if x in vocab]\n",
    "\n",
    "def vocab_decode(array, vocab):\n",
    "    return ''.join([vocab[x - 1] for x in array])\n",
    "\n",
    "def read_data(filename, vocab, window, overlap):\n",
    "    lines = [line.strip() for line in open(filename, 'r').readlines()]\n",
    "    while True:\n",
    "        random.shuffle(lines)\n",
    "\n",
    "        for text in lines:\n",
    "            text = vocab_encode(text, vocab)\n",
    "            for start in range(0, len(text) - window, overlap):\n",
    "                chunk = text[start: start + window]\n",
    "                chunk += [0] * (window - len(chunk))\n",
    "                yield chunk\n",
    "\n",
    "def read_batch(stream, batch_size):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(object):\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.path = model + '.txt'\n",
    "        if 'trump' in model:\n",
    "            self.vocab = (\"'(),-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "                    \" '\\\"_abcdefghijklmnopqrstuvwxyz{|}@#âž¡ðŸ“ˆ\")\n",
    "        else:\n",
    "            self.vocab = (\" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "                    \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "\n",
    "        self.seq = tf.placeholder(tf.int32, [None, None])\n",
    "        self.temp = tf.constant(1.5)\n",
    "        self.hidden_sizes = [128, 256]\n",
    "        self.batch_size = 64\n",
    "        self.lr = 0.0003\n",
    "        self.skip_step = 500\n",
    "        self.num_steps = 50 # for RNN unrolled\n",
    "        self.len_generated = 200\n",
    "        self.gstep = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "\n",
    "    def create_rnn(self, seq):\n",
    "        layers = [tf.nn.rnn_cell.GRUCell(size) for size in self.hidden_sizes]\n",
    "        cells = tf.nn.rnn_cell.MultiRNNCell(layers)\n",
    "        batch = tf.shape(seq)[0]\n",
    "        zero_states = cells.zero_state(batch, dtype=tf.float32)\n",
    "        self.in_state = tuple([tf.placeholder_with_default(state, [None, state.shape[1]]) \n",
    "                                for state in zero_states])\n",
    "        # this line to calculate the real length of seq\n",
    "        # all seq are padded to be of the same length, which is num_steps\n",
    "        length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "        self.output, self.out_state = tf.nn.dynamic_rnn(cells, seq, length, self.in_state)\n",
    "\n",
    "    def create_model(self):\n",
    "        seq = tf.one_hot(self.seq, len(self.vocab))\n",
    "        self.create_rnn(seq)\n",
    "        self.logits = tf.layers.dense(self.output, len(self.vocab), None)\n",
    "        loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=self.logits[:, :-1], \n",
    "                                                        labels=seq[:, 1:])\n",
    "        self.loss = tf.reduce_sum(loss)\n",
    "        # sample the next character from Maxwell-Boltzmann Distribution \n",
    "        # with temperature temp. It works equally well without tf.exp\n",
    "        self.sample = tf.multinomial(tf.exp(self.logits[:, -1] / self.temp), 1)[:, 0] \n",
    "        self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss, global_step=self.gstep)\n",
    "\n",
    "    def train(self):\n",
    "        saver = tf.train.Saver()\n",
    "        start = time.time()\n",
    "        min_loss = None\n",
    "        with tf.Session() as sess:\n",
    "            writer = tf.summary.FileWriter('output', sess.graph)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            ckpt = tf.train.get_checkpoint_state(os.path.dirname('models/' + self.model + '/checkpoint'))\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "            \n",
    "            iteration = self.gstep.eval()\n",
    "            stream = read_data(self.path, self.vocab, self.num_steps, overlap=self.num_steps//2)\n",
    "            data = read_batch(stream, self.batch_size)\n",
    "            while True:\n",
    "                batch = next(data)\n",
    "\n",
    "            # for batch in read_batch(read_data(DATA_PATH, vocab)):\n",
    "                batch_loss, _ = sess.run([self.loss, self.opt], {self.seq: batch})\n",
    "                if (iteration + 1) % self.skip_step == 0:\n",
    "                    print('Iter {}. \\n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "                    self.online_infer(sess)\n",
    "                    start = time.time()\n",
    "                    checkpoint_name = 'models/' + self.model + '/char-rnn'\n",
    "                    if min_loss is None:\n",
    "                        saver.save(sess, checkpoint_name, iteration)\n",
    "                    elif batch_loss < min_loss:\n",
    "                        saver.save(sess, checkpoint_name, iteration)\n",
    "                        min_loss = batch_loss\n",
    "                iteration += 1\n",
    "\n",
    "    def online_infer(self, sess):\n",
    "        \"\"\" Generate sequence one character at a time, based on the previous character\n",
    "        \"\"\"\n",
    "        for seed in ['Hillary', 'I', 'R', 'T', '@', 'N', 'M', '.', 'G', 'A', 'W']:\n",
    "            sentence = seed\n",
    "            state = None\n",
    "            for _ in range(self.len_generated):\n",
    "                batch = [vocab_encode(sentence[-1], self.vocab)]\n",
    "                feed = {self.seq: batch}\n",
    "                if state is not None: # for the first decoder step, the state is None\n",
    "                    for i in range(len(state)):\n",
    "                        feed.update({self.in_state[i]: state[i]})\n",
    "                index, state = sess.run([self.sample, self.out_state], feed)\n",
    "                sentence += vocab_decode(index, self.vocab)\n",
    "            print('\\t' + sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from models/trump_tweets/char-rnn-3299\n",
      "Iter 3499. \n",
      "    Loss 5714.05078125. Time 36.262547969818115\n",
      "\tHillary the great have a great of the will be a many and a really and the seed and the make a great proble to President of the Fake American #Trump2016 __HTTP__ __HTTP__ __HTTP__ __HTTP__ __HTTP__ _EHTThank \n",
      "\tI was the really don't every in the seed don't was a great be and the and a great look to the has and very and the great start and a great and a companies and the for the start of the campaign to see o\n",
      "\tRe start to be the seart to be the one of the deal be and not a proble will be a be and the the campaign to get and the Demack and have a country and the Erec is a start to see lost and for the for the\n",
      "\tThe prome the country and a proble to the U.S. will be a great will be a many to the seed you can the sears so be the make the proble and Obama and have a great the proble and a start to be a many to t\n",
      "\t@bamacare is a complest the really and for the beal and with she can the really and start of the seed and the was so be a made and a great and the your a great and the beal and $10000 mistive and the p\n",
      "\tNe will be a great will be a start to the your sount to the 10 peaple and the start to be the start to be a proble in the New Care and the your a great the tome and the seed to our country and with a g\n",
      "\tMing the seart to be a be and a complest the U.S. President of the 10 the leaders of the seart to see lost be a wand to be a start to be the Erec is a companies and the .... to be a many and of the see\n",
      "\t. will be a start and the beal and the proble to the 6. #Trump2016 __HTTP__ __HTTP__ __HTTP__ __HTTP__ __HTTP__ _EHTP__HTP__HTP__HTP__HTP__HTP__HTP__HTP__HTP__HTP__HTP__HTP__HTP__HTP__HTP__HTP__HTP__HT\n",
      "\tGonald Trump America should be a really and start will be a be and @Mary and the seart and will be a companing the was a compenter and start of the really and a made to be a companing the campaign to m\n",
      "\tA have a great We will be a complest deal and start to a proble in the campaign to a prosident and the Farres to see in the Trump and the Wasting to for the make a great and have be and the beal and wi\n",
      "\tWe will be a be and the was in the New Hall and have be and of the will be the proble to the make a country and the seed and start of the in the proble will be a be and a complest the beal the election\n",
      "Iter 3999. \n",
      "    Loss 5374.5458984375. Time 79.67051482200623\n",
      "\tHillary the have the vitions to the Make America from %our country in the start to make the beat should be a great the will be a great the dound the just really in the proble the American country to the Bast\n",
      "\tI will be a trump in the start to controlled the many the world be a many to the many the really and have the many to the U.S. the Congress and with me in the @MittRomney and start he will be a better \n",
      "\tRo should be a great will be a start to be the U.S. and the New State in the Will will be the X. Sonight and the election of the lost endicted the election of the courted the really start to the hast t\n",
      "\tThe one of the 30 million of the really be a start to get the world be a to be a light the courted the courted and I will be a to me the lost the start to be a start of the Sentice of the one of the do\n",
      "\t@nathenghand @Trump2016 __HTTP__ __HTTP__ __HTTP__ _EHTP__HTP__ETThe Barack Trump Parting job so many the many the great good the Demack is not be and with me the courter and the world be a better the \n",
      "\tNe will be a many to be the beat so be the proted the -ust people of the better the the many the many a great country in the @MittRomney and me the hast in the courted the courter and the Wall be a new\n",
      "\tMericans of the courted the dound the not the start to the proted the Collers and 10 million and the many the start to de and 7 mo is a deal and the ). It is a Trump Trump Trump State of the election o\n",
      "\t. @Chinatorion to see will be a start to the Americans and the really and what the many the beat of the really be a many to the lost start to the Collers will be the proted the U.S. and not in the many\n",
      "\tGonald Trump Congress Quection to will be the courted the U.S. and the for the Forreal Viter has a start to the the many the Great  the many the proted the courted the haster the the beal in the lost t\n",
      "\tA should be a many to the lost and the Frictor on @ApprenticeNBC don't will be a start in the lost in the Collers in the U.S. complest the President the dound the ðŸ“ˆNew York of the great me will be a ma\n",
      "\tWe the under the Barked Obama and the proted the proted the many the many the beat has the courted the start to the lost with the one of the better the really of the lost with Obama is a great a make a\n",
      "Iter 4499. \n",
      "    Loss 5380.4951171875. Time 81.04744100570679\n",
      "\tHillary will be a tramp of the Barter the for the protest the make the protect the was the the people who was the best @BarackObama is a many to the U.S. will be a great and the start to the election to the \n",
      "\tI will be the Great and the to me the to the U.S. (cont) __HTTP__ __HTTP__ __HTTP__ __HTTP__ __HTTP__ __HTTP__ __HTTP__ __HTTP__ __HTTP__ __HTTP__ __HTTP__ __HTTP__ __HTTP__ __HTTP__ __HTTP__ __HTTP__ \n",
      "\tRe start the protection in the start to the best and the to the the was a complayed the U.S. people of the media will be a prople and the best and of the people of the really be a better the was a comp\n",
      "\tThe start the proted the one of the prople that the was a great and the many to the people who was the protect the protest the was a great be a better the has new the many to the American the make the \n",
      "\t@warker is a trampered to the for the great and with the promest the make a great /BarackObama is a lost with the protect the for the protest the was a great and the protect the was a great and the can\n",
      "\tNe will be a better the really the protest the new me the protest the protest the Hell be the election to the prople that the New York of the American under the start to the best the @BarackObama will \n",
      "\tMericans and the can the proted the protect the keep the make the one of the U.S. and the = country and the leaders and the canding the best and just the for the proted the protect the election in the \n",
      "\t. and with the best the the make the Republicans and the new the proted the election of the U.S. and the Colf is a many to the protest the Your every and the for the make the prople will be the protect\n",
      "\tGotal of the really will be the proted the was a lost the make the 10 million to the can the ,artan will be a tramp of the |.S. Part with the U.S. and the Democrats and country and the has been the was\n",
      "\tAmerican for the @BarackObama should be a tramp of the was the protect the country in the @BarackObama should be a great be and the start to the U.S. Enther the Scotland to will be a start to the prote\n",
      "\tWe to the U.S. the country and the just the protest the protect the U.S. will be a great the was a great country and the leaders and with the protest the proted the protest the U.S. will be a great the\n"
     ]
    }
   ],
   "source": [
    "model = 'trump_tweets'\n",
    "\n",
    "lm = CharRNN(model)\n",
    "lm.create_model()\n",
    "lm.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
